{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ariel university\n",
    "\n",
    "<h3><center>Course name: Deep learning and natural language processing</center></h3>\n",
    "\n",
    "Course number: 7061510-1\n",
    "\n",
    "Lecturer: Dr. Amos Azaria\n",
    "\n",
    "The editor: Moshe Hanukoglu\n",
    "\n",
    "Date: First Semester 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>\n",
    "### Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Description Of The Network Structure](#description_of_the_network_structure)\n",
    "    * [Long Short Term Memory (LSTM)](#long_short_term_memory)\n",
    "        * [How Many Weights In LSTM](#how_many_weights_in_lstm)\n",
    "    * [GRU](#gru)\n",
    "* [Generating Text in TensorFlow With RNN](#generating_text_in_tensorflow_with_rnn)\n",
    "* [tf.nn.rnn_cell](#tf.nn.rnn_cell)\n",
    "* [Multiple Layers Of RNN](#multiple_layers_of_rnn)\n",
    "* [BLEU (Bilingual Evaluation Understudy) ScoreRNN](#bleu_scorernn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Introduction\n",
    "<a id='introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analysis of sentences is used in many fields, such as translating one language into another, writing a description of pictures, analysis of sound, etc. In order for us to perform the task properly and we can understand and create new sentences we must understand the content of the entire text and not just check what words appear in it.\n",
    "<br>In this tutorial we will focus on the analysis of text by Recurrent Neural Networks (RNN).\n",
    "<br>On the face of it, we can ask why not use a fully-connected network to analyze the trial?\n",
    "In order to use fully-connected we will need to convert the sentence into a form that we can insert into the network.\n",
    "<br>One method is a bag of words. This method is not a good method because in this method we do not give importance to the order of the words within the sentence, and the order of the words in the sentence is very critical for proper understanding of the meaning of the sentence.\n",
    "\n",
    "For example:\n",
    "1. The child ate pizza and did not eat vegetables\n",
    "2. The child ate vegetables and did not eat pizza\n",
    "\n",
    "The meaning of these two sentences is the opposite of each other, but if we implement the bag of words method for both sentences we will get the same result.\n",
    "<br>Therefore we can not use this method.\n",
    "\n",
    "And the use of the one hot method to represent the words, which gives importance to the location of the words in the sentence, is incorrect because then we do not give importance to the meaning of the words but give all the words the same weight.\n",
    "\n",
    "Another way to try is to use CNN, since CNN gives importance to information points that are close to each other. But still do not receive general evidence of the entire trial.\n",
    "\n",
    "Therefore, as noted above, we will learn a new method for analyzing text called RNN.\n",
    "\n",
    "As we mentioned above, analysis of text can be in many cases. We will detail these cases:\n",
    "\n",
    "1. One to one - The input is single (single image or word) that is classified into a single class (binary classification). For example: Is it home or not?\n",
    "2. One to many - The input is single (image) that is categorized into many departments or written (in words) to the image. For example: for this image <img src=\"two_birds.jpg\" width=\"150px\"/>\n",
    "[credit](https://he.wikipedia.org/wiki/%D7%A4%D7%95%D7%A8%D7%98%D7%9C:%D7%91%D7%A2%D7%9C%D7%99_%D7%97%D7%99%D7%99%D7%9D)\n",
    "    <br>We want it to say: Two birds fly.\n",
    "3. many to one - The input is a group (images or words) that is classified into a single class (binary classification). For example: Does the video (group of pictures) show a birthday party or not, is the sentence urgent or not.\n",
    "\n",
    "4. Many-to-Many Input is a group (images or words) that is converted to another group.\n",
    "   * Translation of sentences. The text is first decoded and only then is the answer given to the output.\n",
    "   * Automatic speech recognition. Speech is deciphered directly into text, you hear a word and translate it and then another word and translate it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Description of the network structure\n",
    "<a id='description_of_the_network_structure'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN structure is: In each iteration get the next character / word $(x_i)$ and the memory of all the so-called until now. The output is the next character / word or the translation / meaning $(h_i)$ of the text so far, the output is also input to the next iteration.\n",
    "<br>Each of the iterations is represented by a square that is written inside it.\n",
    "<br>The two structures are identical, but the right structure is the non-rolled form of the left structure.\n",
    "\n",
    "<img src=\"RNN-unrolled.png\" width=\"450px\"/>\n",
    "\n",
    "[credit](http://colah.github.io/posts/2015-08-Understanding-LSTMs)\n",
    "\n",
    "After we have seen the general structure of all iterations, we will now focus on the internal structure of each iterations.\n",
    "\n",
    "After we have seen the general structure of all iterations, we will now focus on the internal structure of each iterations.\n",
    "\n",
    "RNN has several different implementations\n",
    "* Long Short Term Memory (LSTM)\n",
    "* GRU\n",
    "\n",
    "We will explain the implementation of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "### Long Short Term Memory (LSTM)\n",
    "<a id='long_short_term_memory'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full appearance of the internal structure of each iteration is \n",
    "\n",
    "<img src=\"IntenalLSTM.png\" width=\"700px\"/>\n",
    "\n",
    "[credit](http://colah.github.io/posts/2015-08-Understanding-LSTMs)\n",
    "\n",
    "Now we will explain each and every part of the above structure:\n",
    "\n",
    "* The red part: Forget what has been learned until now. That is, doubling the \"memory\" in the number from 0 to 1 when 0 means forgetting everything learned until now and 1 means remembering everything that has been learned until now, each number in this range means a few percent to remember from what has been learned so far.\n",
    "The sign $\\sigma$ symbolizes logistic regression. The input is the previous memory and the new word (which is not yet read) and the output is a number between 0 and 1 as described above.\n",
    "* Green part: remember the new thing learned (in addition to the old memory) and if so, how much of it to remember, that is, what importance to give to the new thing. That is, this part is responsible for adding memory to the long-term memory.\n",
    "    This section consists of two parts:\n",
    "    1. Logistic regression that determines whether to remember the new thing. His input is the old memory and the new word and his output is a number between 0 and 1 that says whether to remember.\n",
    "    2. Creates the new memory and adds it to the old memory.\n",
    "* Blue part: says how much of all memory created at this stage should move on. We put the memory into the tanh function to get numbers between -1 and 1. The result will be the input of the next step. That is, this part is responsible for short-term memory, the memory that goes to the next stage.\n",
    "\n",
    "\n",
    "In order to expand your understanding of the subject you can read on [this web](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a great web that explains the entire structure very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "#### How Many Weights In LSTM?\n",
    "<a id='how_many_weights_in_lstm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important parameters in understanding the system structure is knowing how many weights there are in the system.\n",
    "To do this we will perform an analysis of the amount of weights in the system.\n",
    "<br>Let's mark a few things:\n",
    "* k - The dimension of the new input $(x_i)$\n",
    "* d - the dimension of the memory transferred from one stage to another (the width of the horizontal lines)\n",
    "* t - like steps / cells\n",
    "* m - Mini-batch size\n",
    "\n",
    "In this implementation, we perform 3 times linear regression and one time tanh, each function is performed d times (for each memory cell). For each of the functions the inputs are entered, $x_i$ (k weights) and the memory from the previous step (d weights), that is there are k + d weights and d d times.\n",
    "So the total account is: $\\left(\\left(k + d\\right) + 1 \\right) * 4 * d$\n",
    "<br>It can be seen that m and t do not affect the number at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "### GRU\n",
    "<a id='gru'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different method for realizing RNN is called GRU. The realization of this method is shown in the image below.\n",
    "This method is similar in its parts to the previous method (LSTM) but has joined several parts together.\n",
    "\n",
    "<img src=\"LSTM3-var-GRU.png\" width=\"300px\"/>\n",
    "\n",
    "[credit](http://colah.github.io/posts/2015-08-Understanding-LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Generating Text in TensorFlow With RNN\n",
    "<a id='generating_text_in_tensorflow_with_rnn'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list that holds all characters as ASCII values\n",
    "all_text = [ord(x) for x in list(urllib.request.urlopen(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\").read().decode(\"utf-8\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "num_past_characters = 10 # The number of previous characters that predict the next character\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list that holds \"representative\" for each ASCII value in the text, \n",
    "# that is, if there are ASCII values in the text: 65,65,81,70,81 then this list will hold 65,81,70\n",
    "char_mapping = list(set(all_text))\n",
    "\n",
    "max_char = max(char_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_chars = len(char_mapping) #The number of different possible text characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_char_map = np.zeros(max_char+1, dtype=int)\n",
    "for i in range(possible_chars):\n",
    "    inverse_char_map[char_mapping[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_examples = int((len(all_text)-(num_past_characters+1))/step)\n",
    "curr_batch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data for the train.\n",
    "<br>In data_x there are $|batch\\_size|$ rows in each of them have $|num\\_past\\_characters|$ characters and each character is represented by the one hot method, meaning that each character is represented by an array of the size of $|possible\\_chars|$\n",
    "<br>In data_y there are the characters that each of the sentences should predict, which means that each sentence has one character represented in the one hot method, so the size of data_y is $|batch\\_size|$ rows and $|possible\\_chars|$ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch():\n",
    "    global curr_batch\n",
    "    curr_range_start = curr_batch*batch_size\n",
    "    if curr_range_start + batch_size >= num_input_examples:\n",
    "        return (None, None)\n",
    "    data_x = np.zeros(shape=(batch_size,num_past_characters,possible_chars))\n",
    "    data_y = np.zeros(shape=(batch_size,possible_chars))\n",
    "    for ep in range(batch_size):\n",
    "        for inc in range(num_past_characters):\n",
    "            data_x[ep][inc][inverse_char_map[all_text[curr_range_start+ep*step+inc]]] = 1\n",
    "        data_y[ep][inverse_char_map[all_text[curr_range_start+ep*step+num_past_characters]]] = 1\n",
    "    curr_batch += 1\n",
    "    return (data_x, data_y)\n",
    "\n",
    "def back_to_text(to_convert): # gets a one-hot encoded matrix and returns text. We'll need this at the end\n",
    "    return [chr(char_mapping[np.argmax(letter)]) for letter in to_convert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 10, 84)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 84)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(a[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 44, 31, 32, 27, 29, 31,  1,  1,  1],\n",
       "       [44, 31, 32, 27, 29, 31,  1,  1,  1, 45],\n",
       "       [31, 32, 27, 29, 31,  1,  1,  1, 45, 47],\n",
       "       [32, 27, 29, 31,  1,  1,  1, 45, 47, 42],\n",
       "       [27, 29, 31,  1,  1,  1, 45, 47, 42, 42],\n",
       "       [29, 31,  1,  1,  1, 45, 47, 42, 42, 41],\n",
       "       [31,  1,  1,  1, 45, 47, 42, 42, 41, 45],\n",
       "       [ 1,  1,  1, 45, 47, 42, 42, 41, 45, 35],\n",
       "       [ 1,  1, 45, 47, 42, 42, 41, 45, 35, 40],\n",
       "       [ 1, 45, 47, 42, 42, 41, 45, 35, 40, 33],\n",
       "       [45, 47, 42, 42, 41, 45, 35, 40, 33,  4],\n",
       "       [47, 42, 42, 41, 45, 35, 40, 33,  4, 75],\n",
       "       [42, 42, 41, 45, 35, 40, 33,  4, 75, 63],\n",
       "       [42, 41, 45, 35, 40, 33,  4, 75, 63, 56],\n",
       "       [41, 45, 35, 40, 33,  4, 75, 63, 56, 75]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellsize = 30\n",
    "x = tf.placeholder(tf.float32, [None, num_past_characters, possible_chars])\n",
    "y = tf.placeholder(tf.float32, [None, possible_chars])\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(cellsize, forget_bias=0.0)\n",
    "output, _ = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = output[-1]\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([cellsize, possible_chars], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0.1, shape=[possible_chars]))\n",
    "\n",
    "z = tf.matmul(last, W) + b\n",
    "res = tf.nn.softmax(z)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(res), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.37731\n",
      "step 1, training accuracy 0.462628\n",
      "step 2, training accuracy 0.488142\n",
      "step 3, training accuracy 0.502892\n",
      "step 4, training accuracy 0.511578\n",
      "step 5, training accuracy 0.517328\n",
      "step 6, training accuracy 0.522003\n",
      "step 7, training accuracy 0.526037\n",
      "step 8, training accuracy 0.529447\n",
      "step 9, training accuracy 0.532766\n",
      "step 10, training accuracy 0.53553\n",
      "step 11, training accuracy 0.537953\n",
      "step 12, training accuracy 0.539881\n",
      "step 13, training accuracy 0.541593\n",
      "step 14, training accuracy 0.54284\n",
      "step 15, training accuracy 0.544211\n",
      "step 16, training accuracy 0.545166\n",
      "step 17, training accuracy 0.546002\n",
      "step 18, training accuracy 0.547057\n",
      "step 19, training accuracy 0.547824\n",
      "step 20, training accuracy 0.548392\n",
      "step 21, training accuracy 0.548999\n",
      "step 22, training accuracy 0.549583\n",
      "step 23, training accuracy 0.550136\n",
      "step 24, training accuracy 0.55063\n",
      "step 25, training accuracy 0.551269\n",
      "step 26, training accuracy 0.551648\n",
      "step 27, training accuracy 0.552025\n",
      "step 28, training accuracy 0.552595\n",
      "step 29, training accuracy 0.552988\n",
      "step 30, training accuracy 0.553251\n",
      "step 31, training accuracy 0.553356\n",
      "step 32, training accuracy 0.553636\n",
      "step 33, training accuracy 0.553792\n",
      "step 34, training accuracy 0.554205\n",
      "step 35, training accuracy 0.554285\n",
      "step 36, training accuracy 0.554439\n",
      "step 37, training accuracy 0.554719\n",
      "step 38, training accuracy 0.554885\n",
      "step 39, training accuracy 0.555198\n",
      "step 40, training accuracy 0.555388\n",
      "step 41, training accuracy 0.555401\n",
      "step 42, training accuracy 0.555629\n",
      "step 43, training accuracy 0.555686\n",
      "step 44, training accuracy 0.555916\n",
      "step 45, training accuracy 0.555851\n",
      "step 46, training accuracy 0.555869\n",
      "step 47, training accuracy 0.556132\n",
      "step 48, training accuracy 0.556205\n",
      "step 49, training accuracy 0.556437\n",
      "step 50, training accuracy 0.556561\n",
      "step 51, training accuracy 0.556904\n",
      "step 52, training accuracy 0.557059\n",
      "step 53, training accuracy 0.557011\n",
      "step 54, training accuracy 0.557224\n",
      "step 55, training accuracy 0.557062\n",
      "step 56, training accuracy 0.557164\n",
      "step 57, training accuracy 0.557267\n",
      "step 58, training accuracy 0.557274\n",
      "step 59, training accuracy 0.557417\n",
      "step 60, training accuracy 0.557568\n",
      "step 61, training accuracy 0.55768\n",
      "step 62, training accuracy 0.557816\n",
      "step 63, training accuracy 0.557928\n",
      "step 64, training accuracy 0.558011\n",
      "step 65, training accuracy 0.557978\n",
      "step 66, training accuracy 0.557981\n",
      "step 67, training accuracy 0.558142\n",
      "step 68, training accuracy 0.558219\n",
      "step 69, training accuracy 0.558251\n",
      "step 70, training accuracy 0.55843\n",
      "step 71, training accuracy 0.558402\n",
      "step 72, training accuracy 0.558454\n",
      "step 73, training accuracy 0.558292\n",
      "step 74, training accuracy 0.558287\n",
      "step 75, training accuracy 0.558367\n",
      "step 76, training accuracy 0.558557\n",
      "step 77, training accuracy 0.558537\n",
      "step 78, training accuracy 0.55849\n",
      "step 79, training accuracy 0.558588\n",
      "step 80, training accuracy 0.558587\n",
      "step 81, training accuracy 0.55883\n",
      "step 82, training accuracy 0.55848\n",
      "step 83, training accuracy 0.558543\n",
      "step 84, training accuracy 0.558395\n",
      "step 85, training accuracy 0.55851\n",
      "step 86, training accuracy 0.558632\n",
      "step 87, training accuracy 0.558608\n",
      "step 88, training accuracy 0.558608\n",
      "step 89, training accuracy 0.558836\n",
      "step 90, training accuracy 0.558645\n",
      "step 91, training accuracy 0.559054\n",
      "step 92, training accuracy 0.558751\n",
      "step 93, training accuracy 0.558965\n",
      "step 94, training accuracy 0.558891\n",
      "step 95, training accuracy 0.558735\n",
      "step 96, training accuracy 0.558698\n",
      "step 97, training accuracy 0.558948\n",
      "step 98, training accuracy 0.559004\n",
      "step 99, training accuracy 0.558813\n",
      "step 100, training accuracy 0.558732\n",
      "step 101, training accuracy 0.558795\n",
      "step 102, training accuracy 0.558975\n",
      "step 103, training accuracy 0.558973\n",
      "step 104, training accuracy 0.558603\n",
      "step 105, training accuracy 0.558836\n",
      "step 106, training accuracy 0.558953\n",
      "step 107, training accuracy 0.559061\n",
      "step 108, training accuracy 0.558738\n",
      "step 109, training accuracy 0.559118\n",
      "step 110, training accuracy 0.559111\n",
      "step 111, training accuracy 0.559377\n",
      "step 112, training accuracy 0.558855\n",
      "step 113, training accuracy 0.559174\n",
      "step 114, training accuracy 0.559354\n",
      "step 115, training accuracy 0.559251\n",
      "step 116, training accuracy 0.559209\n",
      "step 117, training accuracy 0.559214\n",
      "step 118, training accuracy 0.559211\n",
      "step 119, training accuracy 0.55954\n",
      "step 120, training accuracy 0.559377\n",
      "step 121, training accuracy 0.559562\n",
      "step 122, training accuracy 0.559599\n",
      "step 123, training accuracy 0.559445\n",
      "step 124, training accuracy 0.559645\n",
      "step 125, training accuracy 0.55957\n",
      "step 126, training accuracy 0.559414\n",
      "step 127, training accuracy 0.559607\n",
      "step 128, training accuracy 0.559644\n",
      "step 129, training accuracy 0.559855\n",
      "step 130, training accuracy 0.559725\n",
      "step 131, training accuracy 0.559582\n",
      "step 132, training accuracy 0.559377\n",
      "step 133, training accuracy 0.559712\n",
      "step 134, training accuracy 0.55957\n",
      "step 135, training accuracy 0.559738\n",
      "step 136, training accuracy 0.559887\n",
      "step 137, training accuracy 0.559988\n",
      "step 138, training accuracy 0.560085\n",
      "step 139, training accuracy 0.559996\n",
      "step 140, training accuracy 0.560116\n",
      "step 141, training accuracy 0.560246\n",
      "step 142, training accuracy 0.560043\n",
      "step 143, training accuracy 0.560161\n",
      "step 144, training accuracy 0.560298\n",
      "step 145, training accuracy 0.559973\n",
      "step 146, training accuracy 0.560374\n",
      "step 147, training accuracy 0.560348\n",
      "step 148, training accuracy 0.560374\n",
      "step 149, training accuracy 0.560201\n",
      "step 150, training accuracy 0.560274\n",
      "step 151, training accuracy 0.560361\n",
      "step 152, training accuracy 0.560303\n",
      "step 153, training accuracy 0.560234\n",
      "step 154, training accuracy 0.560314\n",
      "step 155, training accuracy 0.560479\n",
      "step 156, training accuracy 0.560256\n",
      "step 157, training accuracy 0.560396\n",
      "step 158, training accuracy 0.560595\n",
      "step 159, training accuracy 0.560402\n",
      "step 160, training accuracy 0.560536\n",
      "step 161, training accuracy 0.560346\n",
      "step 162, training accuracy 0.560246\n",
      "step 163, training accuracy 0.560331\n",
      "step 164, training accuracy 0.560296\n",
      "step 165, training accuracy 0.56061\n",
      "step 166, training accuracy 0.560652\n",
      "step 167, training accuracy 0.560326\n",
      "step 168, training accuracy 0.560549\n",
      "step 169, training accuracy 0.56062\n",
      "step 170, training accuracy 0.560875\n",
      "step 171, training accuracy 0.560782\n",
      "step 172, training accuracy 0.560532\n",
      "step 173, training accuracy 0.560789\n",
      "step 174, training accuracy 0.56075\n",
      "step 175, training accuracy 0.560532\n",
      "step 176, training accuracy 0.560344\n",
      "step 177, training accuracy 0.560539\n",
      "step 178, training accuracy 0.560867\n",
      "step 179, training accuracy 0.560705\n",
      "step 180, training accuracy 0.56087\n",
      "step 181, training accuracy 0.560667\n",
      "step 182, training accuracy 0.560742\n",
      "step 183, training accuracy 0.560915\n",
      "step 184, training accuracy 0.56067\n",
      "step 185, training accuracy 0.560372\n",
      "step 186, training accuracy 0.560794\n",
      "step 187, training accuracy 0.560474\n",
      "step 188, training accuracy 0.56082\n",
      "step 189, training accuracy 0.560566\n",
      "step 190, training accuracy 0.560391\n",
      "step 191, training accuracy 0.560544\n",
      "step 192, training accuracy 0.560692\n",
      "step 193, training accuracy 0.560615\n",
      "step 194, training accuracy 0.560572\n",
      "step 195, training accuracy 0.560346\n",
      "step 196, training accuracy 0.560301\n",
      "step 197, training accuracy 0.560509\n",
      "step 198, training accuracy 0.560501\n",
      "step 199, training accuracy 0.560504\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(res,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "num_of_epochs = 200    \n",
    "for ephoch in range(num_of_epochs):\n",
    "    acc = 0\n",
    "    curr_batch = 0\n",
    "    while True:\n",
    "        batch_xs, batch_ys = next_batch()\n",
    "        if batch_xs is None:\n",
    "            break\n",
    "        else:\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            acc += accuracy.eval(feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "    print(\"step %d, training accuracy %g\"%(ephoch, acc/curr_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the sense of the \n"
     ]
    }
   ],
   "source": [
    "def text2arr(source, start):\n",
    "    src_as_num = [ord(x) for x in source]\n",
    "    ret_arr = np.zeros(shape=(1,num_past_characters,possible_chars))\n",
    "    for inc in range(num_past_characters):\n",
    "        ret_arr[0][inc][inverse_char_map[src_as_num[start+inc]]] = 1\n",
    "    return ret_arr\n",
    "\n",
    "requested_length = 200\n",
    "text = list(\"hello world\")\n",
    "for i in range(requested_length):\n",
    "    predicted_letter = back_to_text(res.eval(feed_dict={x:text2arr(text, len(text)-num_past_characters)}))\n",
    "    text += predicted_letter\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems caused by predicting text can be that you enter a loop and then the text is duplicated and does not produce new text but repeats existing text. This problem is created once the characters that guess the next character are repeated.\n",
    "In order to solve this problem, we will drag the following character according to the softmax returns, which means that a character with a high probability in softmax will be more likely to be the next character, in such a way that the probability of receiving a sequence of characters is the same as the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "### tf.nn.rnn_cell\n",
    "<a id='tf.nn.rnn_cell'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.nn.rnn_cell.BasicLSTMCell.__init__(num_units, forget_bias=1.0, state_is_tuple=True, activation=tanh)\n",
    "```\n",
    "Documentation of the function in this [link](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicLSTMCell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "### Multiple Layers Of RNN\n",
    "<a id='multiple_layers_of_rnn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could take a single neuron and produce from it a grid with several layers so we can also produce a grid with several layers of RNN which means that all h_i will become x_i of the next layer.\n",
    "\n",
    "The code that does this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cells_size = [120, 70, 50]\n",
    "\n",
    "def get_lstm_dropout(cellsize, dropout):\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(cellsize, forget_bias=1.0) \n",
    "       dopoutRNN = tf.nn.rnn_cell.DropoutWrapper(lstm_cell , output_keep_prob=dropout)\n",
    "    return dropoutRNN\n",
    "\n",
    "rnn_cell.MultiRNNCell([get_lstm_dropout(mem_size, dropout) for mem_size in cells_size])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "### BLEU (Bilingual Evaluation Understudy) ScoreRNN\n",
    "<a id='bleu_scorernn'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU is a measure of the quality of translation carried out by machine translation (MT, candidates) in relation to the translation of a professional person (reference sentence), which takes into account the general translation and does not refer to lack of intelligence or grammatical errors in translation.\n",
    "This metric is based on a comparison of candidate for the correct translation of the sentence, reference sentence.\n",
    "The output is a number between 0 and 1 where as close to 1 means that it is closer to the reference sentence.\n",
    "\n",
    "The algorithm:\n",
    "1. Divide the candidate into n-gram units (usually used in 4-gram).\n",
    "2. Sum the number of times each candidate n-gram unit appears in the reference sentence and divide the number of n-gram units in the candidate. (When one unit is checked, the order of the words is not important, but whether or not the words appear)\n",
    "\n",
    "\n",
    "\n",
    "Because of the algorithm's method of operation, it will give a higher score to shorter sentences, whereas in many cases the long sentences are correct and therefore we will multiply the result given by the algorithm by $e^{1-r/c}$, r is the total length of all the reference sentences, c is the total length of all the candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
