{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ariel university\n",
    "\n",
    "<h3><center>Course name: Deep learning and natural language processing</center></h3>\n",
    "\n",
    "Course number: 7061510-1\n",
    "\n",
    "Lecturer: Dr. Amos Azaria\n",
    "\n",
    "The editor: Moshe Hanukoglu\n",
    "\n",
    "Date: First Semester 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>\n",
    "### Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Why use Multilayer Perceptron?](#why_use_multilayer_perceptron)\n",
    "* [Activation Fnction](#activation_function)\n",
    "* [MultiLayer Perceptron For Identifying Distance](#multiLayer_perceptron_for_identifying_distance)\n",
    "* [Predicting Handwritten Numbers](#predicting_handwritten_numbers)\n",
    "* [Resolving Errors](#resolving_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Introduction\n",
    "<a id='introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define some definitions before we begin to engage in this subject in depth.\n",
    "\n",
    "#### Perceptron\n",
    "This is a small unit that has inputs, a function to process the inputs, and an output.\n",
    "\n",
    "This unit looks like this: \n",
    "\n",
    "<img src=\"perceptron.png\" width=\"450px\"/>\n",
    "\n",
    "Part One: Inputs\n",
    "<br>Each input has weight. We denote the input $i$ as $x_i$ and the weight of $x_i$ as $w_i$. Each input can be 0 or 1.\n",
    "Sometimes we add one input called bais and its value will always be 1 and its weight will be marked as $b$.\n",
    "\n",
    "Part Two: Activation function\n",
    "<br>Multiply all its weight and input and sum everything, that is $\\sum_{i = 1}^{n} {w_ix_i}$ and then put the result into a given function. We will talk about the types of functions below.\n",
    "\n",
    "Part Three: Output\n",
    "<br>The output of the unit will be the output given by the function. If this unit is connected to other units (as we will see below) then each of the other units connected to it will receive the output of this unit.\n",
    "\n",
    "#### Multilayer Perceptron (Neural Network, NN)\n",
    "is a structure composed of several perceptron that are connected to each other.\n",
    "\n",
    "This structure looks like this:\n",
    "\n",
    "<img src=\"multi_perceptron.png\" width=\"550px\"/>\n",
    "\n",
    "[credit](https://www.google.co.il/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwi_5unYk9zeAhVCQhoKHd0cAfUQjhx6BAgBEAM&url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FThe-Block-diagram-of-a-three-hidden-layer-multilayer-perceptron-MLP_fig14_303286711&psig=AOvVaw0G7JcOhKNUktopEVbGeAAb&ust=1542568101075988)\n",
    "\n",
    "When each line of perceptron is called a layer, the first layer is called the input layer, the last layer is called the output layer and the other layers are called hidden layers.\n",
    "\n",
    "Note: In cases where all neurons do the same, we initialize the weights to small random values so that we do not get the same result in all neurons.\n",
    "\n",
    "#### Fully-connected\n",
    "Is a multilayer perceptron that each perceptron level $i$ is connected to each perceptron at the $i + 1$ level, as in the image above.\n",
    "\n",
    "#### Feedforward and backpropagation\n",
    "\n",
    "This method is to find the right sounds on the bow so that the output is correct.\n",
    "In linear / logistic regression we looked for the right weights to get the right result when we put in new input, here, too, we want to find the right weights.\n",
    "<br>The difference between the cases is the difficulty in finding the weights, because in linear / logistic regression one group of weights was found that was almost independent of them. Now in neural network you have to find all the weights that are on the arches between the layers and any change in weight affects all the weights that are connected to it.\n",
    "\n",
    "\n",
    "\n",
    "To expand knowledge [link](https://www.science.co.il/moshe/documents/Neural-computation-notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Why use Multilayer Perceptron?\n",
    "<a id='why_use_multilayer_perceptron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, all learning was based on features we knew about data in advance and we taught the model to predict things based solely on the features we knew in advance. And we were able to learn only about these qualities.\n",
    "\n",
    "The use of multilayer perceptron comes to help the model learn about more featurs and also more abstract connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Activation Function\n",
    "<a id='activation_function'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function is one function of a set of functions into which we insert the sum $\\sum_{i = 1}^{n} {w_ix_i}$ and it gives some value other than the sum.\n",
    "\n",
    "The question is why not stay with the amount we received as it is and output it but put it into the activation function?\n",
    "\n",
    "If we do not use the activation function, but each neuron exits the sum $\\sum_{i = 1}^{n} {w_ix_i}$ then if we analyze and open parentheses in the calculation of the final output we find that the final output (for each neuron in the output layer) is from the form $x_i*(Linear\\ combination\\ of\\ weights)$\n",
    "\n",
    "This means that the equation is exactly the same equation as linear regression, and we want to build the neural network so as not to reach the same answer again as linear regression but to another answer.\n",
    "\n",
    "Therefore, in the output of each neuron we perform an activation function in order to \"break\" the linearity.\n",
    "\n",
    "There are many types of activation functions (as can be seen in the [link](https://en.wikipedia.org/wiki/Activation_function)).\n",
    "\n",
    "We'll display three common functions:\n",
    "1. $ReLU(x) = \\left\\{\\begin{array}{ll}x  & x > 0 \\\\0 &  otherwise\\end{array}\\right.$\n",
    "2. $Leaky ReLU(\\alpha,x) = \\left\\{\\begin{array}{ll}x  & x > 0 \\\\\\alpha x &  otherwise\\end{array}\\right.$\n",
    "3. $ELU(\\alpha,x) = \\left\\{\\begin{array}{ll}x  & x > 0 \\\\\\alpha(e^x-1) &  otherwise\\end{array}\n",
    "\\right.$\n",
    "\n",
    "The $Leaky ReLU(\\alpha,x)$ and $ELU(\\alpha,x)$ prevent dead neurons because negative values also get value. Meaning that it will be gradient for any value and we can continue to calculate the values of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## MultiLayer Perceptron For Identifying Distance\n",
    "<a id='multiLayer_perceptron_for_identifying_distance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pairs of numbers and each pair given a label.\n",
    "<br>You have to build a model that could give the right label to a couple of numbers that he had not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "features = 2\n",
    "hidden_layer_nodes = 10\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of the first layer.\n",
    "\n",
    "The truncated_normal function randomly initializes the weights with a normal distribution and dependence on stddev.\n",
    "\n",
    "We give a positive bias to prevent dead neurons, because once a neuron reaches 0, it will always remain with 0 as a result of the ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([features,hidden_layer_nodes], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden_layer_nodes]))\n",
    "z1 = tf.add(tf.matmul(x,W1),b1)\n",
    "a1 = tf.nn.relu(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes,1], stddev=0.1))\n",
    "b2 = tf.Variable(0.)\n",
    "z2 = tf.matmul(a1,W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.sigmoid(z2)\n",
    "loss = tf.reduce_mean(-(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y)))\n",
    "update = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "data_x = np.array([[2,32], [25,1.2], [5,25.2], [23,2], [56,8.5], [60,60], [3,3], [46,53], [3.5,2]])\n",
    "data_y = np.array([[1], [1], [1], [1], [1], [0], [0], [0], [0]])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(0,50000):\n",
    "        sess.run(update, feed_dict = {x:data_x, y_:data_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of this network is 2 neurons in the input layer, 10 neurons in the middle layer (hidden layer) and one neuron in the output layer.\n",
    "\n",
    "We will present the structure of the network in the form of matrices:\n",
    "<br>The addition of the values of b we have not shown, this is only general evidence.\n",
    "\n",
    "<img src=\"NN_Structure.png\" width=\"550px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  [[6.6102937e-02]\n",
      " [9.9863952e-01]\n",
      " [9.9742377e-01]\n",
      " [3.8266489e-01]\n",
      " [1.0349377e-04]]\n"
     ]
    }
   ],
   "source": [
    "print('prediction: ', y.eval(session=sess, feed_dict = \t{x:[[13,12], [0,33], [40,3], [1,1], [50,50]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Predicting Handwritten Numbers\n",
    "<a id='predicting_handwritten_numbers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to begin the prediction we need to teach our model on an existing dataset and for each data point there is a label of the actual number it represents.\n",
    "\n",
    "Dataset's very familiar handwritten numbers is MNIST, a dataset that contains 60,000 samples with labels.\n",
    "\n",
    "Build the NN model and use SoftMax to decide which digit.\n",
    "\n",
    "Source: [link](https://www.tensorflow.org/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iteration:  0  Accuracy: 0.354\n",
      "Iteration:  1  Accuracy: 0.5606\n",
      "Iteration:  2  Accuracy: 0.6813\n",
      "Iteration:  3  Accuracy: 0.7551\n",
      "Iteration:  4  Accuracy: 0.7943\n",
      "Iteration:  5  Accuracy: 0.8169\n",
      "Iteration:  6  Accuracy: 0.834\n",
      "Iteration:  7  Accuracy: 0.8478\n",
      "Iteration:  8  Accuracy: 0.8568\n",
      "Iteration:  9  Accuracy: 0.8623\n",
      "Iteration:  10  Accuracy: 0.8681\n",
      "Iteration:  11  Accuracy: 0.8751\n",
      "Iteration:  12  Accuracy: 0.8803\n",
      "Iteration:  13  Accuracy: 0.8835\n",
      "Iteration:  14  Accuracy: 0.8872\n",
      "Iteration:  15  Accuracy: 0.8889\n",
      "Iteration:  16  Accuracy: 0.8914\n",
      "Iteration:  17  Accuracy: 0.8934\n",
      "Iteration:  18  Accuracy: 0.8961\n",
      "Iteration:  19  Accuracy: 0.898\n",
      "Iteration:  20  Accuracy: 0.8997\n",
      "Iteration:  21  Accuracy: 0.9007\n",
      "Iteration:  22  Accuracy: 0.902\n",
      "Iteration:  23  Accuracy: 0.9036\n",
      "Iteration:  24  Accuracy: 0.905\n",
      "Iteration:  25  Accuracy: 0.9063\n",
      "Iteration:  26  Accuracy: 0.907\n",
      "Iteration:  27  Accuracy: 0.9074\n",
      "Iteration:  28  Accuracy: 0.9077\n",
      "Iteration:  29  Accuracy: 0.9084\n",
      "Iteration:  30  Accuracy: 0.9099\n",
      "Iteration:  31  Accuracy: 0.9113\n",
      "Iteration:  32  Accuracy: 0.9115\n",
      "Iteration:  33  Accuracy: 0.9128\n",
      "Iteration:  34  Accuracy: 0.913\n",
      "Iteration:  35  Accuracy: 0.9134\n",
      "Iteration:  36  Accuracy: 0.9153\n",
      "Iteration:  37  Accuracy: 0.9143\n",
      "Iteration:  38  Accuracy: 0.9155\n",
      "Iteration:  39  Accuracy: 0.9161\n",
      "Iteration:  40  Accuracy: 0.9162\n",
      "Iteration:  41  Accuracy: 0.9168\n",
      "Iteration:  42  Accuracy: 0.9166\n",
      "Iteration:  43  Accuracy: 0.9192\n",
      "Iteration:  44  Accuracy: 0.9192\n",
      "Iteration:  45  Accuracy: 0.9204\n",
      "Iteration:  46  Accuracy: 0.92\n",
      "Iteration:  47  Accuracy: 0.9215\n",
      "Iteration:  48  Accuracy: 0.9209\n",
      "Iteration:  49  Accuracy: 0.9217\n",
      "Iteration:  50  Accuracy: 0.9218\n",
      "Iteration:  51  Accuracy: 0.9232\n",
      "Iteration:  52  Accuracy: 0.9238\n",
      "Iteration:  53  Accuracy: 0.9248\n",
      "Iteration:  54  Accuracy: 0.9239\n",
      "Iteration:  55  Accuracy: 0.9254\n",
      "Iteration:  56  Accuracy: 0.9269\n",
      "Iteration:  57  Accuracy: 0.9267\n",
      "Iteration:  58  Accuracy: 0.9269\n",
      "Iteration:  59  Accuracy: 0.9273\n",
      "Iteration:  60  Accuracy: 0.9275\n",
      "Iteration:  61  Accuracy: 0.9279\n",
      "Iteration:  62  Accuracy: 0.9295\n",
      "Iteration:  63  Accuracy: 0.9303\n",
      "Iteration:  64  Accuracy: 0.9307\n",
      "Iteration:  65  Accuracy: 0.9301\n",
      "Iteration:  66  Accuracy: 0.9309\n",
      "Iteration:  67  Accuracy: 0.9316\n",
      "Iteration:  68  Accuracy: 0.931\n",
      "Iteration:  69  Accuracy: 0.9313\n",
      "Iteration:  70  Accuracy: 0.9316\n",
      "Iteration:  71  Accuracy: 0.9333\n",
      "Iteration:  72  Accuracy: 0.9332\n",
      "Iteration:  73  Accuracy: 0.9331\n",
      "Iteration:  74  Accuracy: 0.9337\n",
      "Iteration:  75  Accuracy: 0.9349\n",
      "Iteration:  76  Accuracy: 0.9342\n",
      "Iteration:  77  Accuracy: 0.936\n",
      "Iteration:  78  Accuracy: 0.9352\n",
      "Iteration:  79  Accuracy: 0.9353\n",
      "Iteration:  80  Accuracy: 0.9366\n",
      "Iteration:  81  Accuracy: 0.9367\n",
      "Iteration:  82  Accuracy: 0.9372\n",
      "Iteration:  83  Accuracy: 0.9369\n",
      "Iteration:  84  Accuracy: 0.9381\n",
      "Iteration:  85  Accuracy: 0.9387\n",
      "Iteration:  86  Accuracy: 0.9379\n",
      "Iteration:  87  Accuracy: 0.9381\n",
      "Iteration:  88  Accuracy: 0.9388\n",
      "Iteration:  89  Accuracy: 0.9384\n",
      "Iteration:  90  Accuracy: 0.939\n",
      "Iteration:  91  Accuracy: 0.9399\n",
      "Iteration:  92  Accuracy: 0.9395\n",
      "Iteration:  93  Accuracy: 0.9404\n",
      "Iteration:  94  Accuracy: 0.9402\n",
      "Iteration:  95  Accuracy: 0.9405\n",
      "Iteration:  96  Accuracy: 0.9409\n",
      "Iteration:  97  Accuracy: 0.9409\n",
      "Iteration:  98  Accuracy: 0.9418\n",
      "Iteration:  99  Accuracy: 0.9419\n",
      "Iteration:  100  Accuracy: 0.9426\n",
      "Iteration:  101  Accuracy: 0.9422\n",
      "Iteration:  102  Accuracy: 0.9422\n",
      "Iteration:  103  Accuracy: 0.9424\n",
      "Iteration:  104  Accuracy: 0.9429\n",
      "Iteration:  105  Accuracy: 0.9428\n",
      "Iteration:  106  Accuracy: 0.9433\n",
      "Iteration:  107  Accuracy: 0.9436\n",
      "Iteration:  108  Accuracy: 0.9439\n",
      "Iteration:  109  Accuracy: 0.9442\n",
      "Iteration:  110  Accuracy: 0.9443\n",
      "Iteration:  111  Accuracy: 0.9451\n",
      "Iteration:  112  Accuracy: 0.9446\n",
      "Iteration:  113  Accuracy: 0.945\n",
      "Iteration:  114  Accuracy: 0.9453\n",
      "Iteration:  115  Accuracy: 0.9456\n",
      "Iteration:  116  Accuracy: 0.945\n",
      "Iteration:  117  Accuracy: 0.9454\n",
      "Iteration:  118  Accuracy: 0.9459\n",
      "Iteration:  119  Accuracy: 0.9463\n",
      "Iteration:  120  Accuracy: 0.9458\n",
      "Iteration:  121  Accuracy: 0.946\n",
      "Iteration:  122  Accuracy: 0.9469\n",
      "Iteration:  123  Accuracy: 0.9468\n",
      "Iteration:  124  Accuracy: 0.9471\n",
      "Iteration:  125  Accuracy: 0.947\n",
      "Iteration:  126  Accuracy: 0.9477\n",
      "Iteration:  127  Accuracy: 0.9478\n",
      "Iteration:  128  Accuracy: 0.9482\n",
      "Iteration:  129  Accuracy: 0.9483\n",
      "Iteration:  130  Accuracy: 0.9485\n",
      "Iteration:  131  Accuracy: 0.949\n",
      "Iteration:  132  Accuracy: 0.9486\n",
      "Iteration:  133  Accuracy: 0.9493\n",
      "Iteration:  134  Accuracy: 0.9492\n",
      "Iteration:  135  Accuracy: 0.9494\n",
      "Iteration:  136  Accuracy: 0.9495\n",
      "Iteration:  137  Accuracy: 0.9503\n",
      "Iteration:  138  Accuracy: 0.95\n",
      "Iteration:  139  Accuracy: 0.9498\n",
      "Iteration:  140  Accuracy: 0.9506\n",
      "Iteration:  141  Accuracy: 0.9508\n",
      "Iteration:  142  Accuracy: 0.9512\n",
      "Iteration:  143  Accuracy: 0.9508\n",
      "Iteration:  144  Accuracy: 0.9512\n",
      "Iteration:  145  Accuracy: 0.951\n",
      "Iteration:  146  Accuracy: 0.9516\n",
      "Iteration:  147  Accuracy: 0.9516\n",
      "Iteration:  148  Accuracy: 0.9518\n",
      "Iteration:  149  Accuracy: 0.9516\n",
      "Iteration:  150  Accuracy: 0.9524\n",
      "Iteration:  151  Accuracy: 0.9518\n",
      "Iteration:  152  Accuracy: 0.9524\n",
      "Iteration:  153  Accuracy: 0.9524\n",
      "Iteration:  154  Accuracy: 0.9525\n",
      "Iteration:  155  Accuracy: 0.9521\n",
      "Iteration:  156  Accuracy: 0.9528\n",
      "Iteration:  157  Accuracy: 0.9532\n",
      "Iteration:  158  Accuracy: 0.9529\n",
      "Iteration:  159  Accuracy: 0.9533\n",
      "Iteration:  160  Accuracy: 0.9534\n",
      "Iteration:  161  Accuracy: 0.9531\n",
      "Iteration:  162  Accuracy: 0.9534\n",
      "Iteration:  163  Accuracy: 0.954\n",
      "Iteration:  164  Accuracy: 0.9542\n",
      "Iteration:  165  Accuracy: 0.9537\n",
      "Iteration:  166  Accuracy: 0.9541\n",
      "Iteration:  167  Accuracy: 0.9543\n",
      "Iteration:  168  Accuracy: 0.9544\n",
      "Iteration:  169  Accuracy: 0.9542\n",
      "Iteration:  170  Accuracy: 0.9543\n",
      "Iteration:  171  Accuracy: 0.955\n",
      "Iteration:  172  Accuracy: 0.955\n",
      "Iteration:  173  Accuracy: 0.9559\n",
      "Iteration:  174  Accuracy: 0.9553\n",
      "Iteration:  175  Accuracy: 0.9556\n",
      "Iteration:  176  Accuracy: 0.9561\n",
      "Iteration:  177  Accuracy: 0.956\n",
      "Iteration:  178  Accuracy: 0.9561\n",
      "Iteration:  179  Accuracy: 0.9562\n",
      "Iteration:  180  Accuracy: 0.9561\n",
      "Iteration:  181  Accuracy: 0.9563\n",
      "Iteration:  182  Accuracy: 0.9559\n",
      "Iteration:  183  Accuracy: 0.957\n",
      "Iteration:  184  Accuracy: 0.9565\n",
      "Iteration:  185  Accuracy: 0.9567\n",
      "Iteration:  186  Accuracy: 0.957\n",
      "Iteration:  187  Accuracy: 0.9565\n",
      "Iteration:  188  Accuracy: 0.9574\n",
      "Iteration:  189  Accuracy: 0.9571\n",
      "Iteration:  190  Accuracy: 0.9572\n",
      "Iteration:  191  Accuracy: 0.9568\n",
      "Iteration:  192  Accuracy: 0.9579\n",
      "Iteration:  193  Accuracy: 0.9576\n",
      "Iteration:  194  Accuracy: 0.9577\n",
      "Iteration:  195  Accuracy: 0.9581\n",
      "Iteration:  196  Accuracy: 0.9579\n",
      "Iteration:  197  Accuracy: 0.958\n",
      "Iteration:  198  Accuracy: 0.958\n",
      "Iteration:  199  Accuracy: 0.9577\n",
      "Iteration:  200  Accuracy: 0.9583\n",
      "Iteration:  201  Accuracy: 0.9583\n",
      "Iteration:  202  Accuracy: 0.9581\n",
      "Iteration:  203  Accuracy: 0.9582\n",
      "Iteration:  204  Accuracy: 0.9586\n",
      "Iteration:  205  Accuracy: 0.9583\n",
      "Iteration:  206  Accuracy: 0.9588\n",
      "Iteration:  207  Accuracy: 0.9587\n",
      "Iteration:  208  Accuracy: 0.9588\n",
      "Iteration:  209  Accuracy: 0.9592\n",
      "Iteration:  210  Accuracy: 0.9593\n",
      "Iteration:  211  Accuracy: 0.9595\n",
      "Iteration:  212  Accuracy: 0.9592\n",
      "Iteration:  213  Accuracy: 0.9594\n",
      "Iteration:  214  Accuracy: 0.9596\n",
      "Iteration:  215  Accuracy: 0.9596\n",
      "Iteration:  216  Accuracy: 0.96\n",
      "Iteration:  217  Accuracy: 0.9602\n",
      "Iteration:  218  Accuracy: 0.9599\n",
      "Iteration:  219  Accuracy: 0.96\n",
      "Iteration:  220  Accuracy: 0.9603\n",
      "Iteration:  221  Accuracy: 0.9601\n",
      "Iteration:  222  Accuracy: 0.9609\n",
      "Iteration:  223  Accuracy: 0.9602\n",
      "Iteration:  224  Accuracy: 0.9607\n",
      "Iteration:  225  Accuracy: 0.9603\n",
      "Iteration:  226  Accuracy: 0.961\n",
      "Iteration:  227  Accuracy: 0.9614\n",
      "Iteration:  228  Accuracy: 0.9616\n",
      "Iteration:  229  Accuracy: 0.961\n",
      "Iteration:  230  Accuracy: 0.9609\n",
      "Iteration:  231  Accuracy: 0.9611\n",
      "Iteration:  232  Accuracy: 0.9618\n",
      "Iteration:  233  Accuracy: 0.961\n",
      "Iteration:  234  Accuracy: 0.9609\n",
      "Iteration:  235  Accuracy: 0.962\n",
      "Iteration:  236  Accuracy: 0.9619\n",
      "Iteration:  237  Accuracy: 0.9612\n",
      "Iteration:  238  Accuracy: 0.9617\n",
      "Iteration:  239  Accuracy: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  240  Accuracy: 0.9617\n",
      "Iteration:  241  Accuracy: 0.9613\n",
      "Iteration:  242  Accuracy: 0.962\n",
      "Iteration:  243  Accuracy: 0.9616\n",
      "Iteration:  244  Accuracy: 0.9619\n",
      "Iteration:  245  Accuracy: 0.9617\n",
      "Iteration:  246  Accuracy: 0.9616\n",
      "Iteration:  247  Accuracy: 0.9622\n",
      "Iteration:  248  Accuracy: 0.9617\n",
      "Iteration:  249  Accuracy: 0.9616\n",
      "Iteration:  250  Accuracy: 0.962\n",
      "Iteration:  251  Accuracy: 0.9623\n",
      "Iteration:  252  Accuracy: 0.9624\n",
      "Iteration:  253  Accuracy: 0.9618\n",
      "Iteration:  254  Accuracy: 0.9621\n",
      "Iteration:  255  Accuracy: 0.9625\n",
      "Iteration:  256  Accuracy: 0.9626\n",
      "Iteration:  257  Accuracy: 0.9625\n",
      "Iteration:  258  Accuracy: 0.9623\n",
      "Iteration:  259  Accuracy: 0.9627\n",
      "Iteration:  260  Accuracy: 0.9624\n",
      "Iteration:  261  Accuracy: 0.9631\n",
      "Iteration:  262  Accuracy: 0.963\n",
      "Iteration:  263  Accuracy: 0.9626\n",
      "Iteration:  264  Accuracy: 0.9629\n",
      "Iteration:  265  Accuracy: 0.9633\n",
      "Iteration:  266  Accuracy: 0.9633\n",
      "Iteration:  267  Accuracy: 0.9628\n",
      "Iteration:  268  Accuracy: 0.9632\n",
      "Iteration:  269  Accuracy: 0.9631\n",
      "Iteration:  270  Accuracy: 0.9635\n",
      "Iteration:  271  Accuracy: 0.9634\n",
      "Iteration:  272  Accuracy: 0.9633\n",
      "Iteration:  273  Accuracy: 0.9637\n",
      "Iteration:  274  Accuracy: 0.9633\n",
      "Iteration:  275  Accuracy: 0.9636\n",
      "Iteration:  276  Accuracy: 0.9633\n",
      "Iteration:  277  Accuracy: 0.9637\n",
      "Iteration:  278  Accuracy: 0.9637\n",
      "Iteration:  279  Accuracy: 0.9638\n",
      "Iteration:  280  Accuracy: 0.9635\n",
      "Iteration:  281  Accuracy: 0.9642\n",
      "Iteration:  282  Accuracy: 0.9637\n",
      "Iteration:  283  Accuracy: 0.9635\n",
      "Iteration:  284  Accuracy: 0.9643\n",
      "Iteration:  285  Accuracy: 0.964\n",
      "Iteration:  286  Accuracy: 0.9642\n",
      "Iteration:  287  Accuracy: 0.9639\n",
      "Iteration:  288  Accuracy: 0.9641\n",
      "Iteration:  289  Accuracy: 0.9646\n",
      "Iteration:  290  Accuracy: 0.9643\n",
      "Iteration:  291  Accuracy: 0.9641\n",
      "Iteration:  292  Accuracy: 0.965\n",
      "Iteration:  293  Accuracy: 0.9649\n",
      "Iteration:  294  Accuracy: 0.965\n",
      "Iteration:  295  Accuracy: 0.9648\n",
      "Iteration:  296  Accuracy: 0.965\n",
      "Iteration:  297  Accuracy: 0.965\n",
      "Iteration:  298  Accuracy: 0.9652\n",
      "Iteration:  299  Accuracy: 0.9652\n",
      "Iteration:  300  Accuracy: 0.965\n",
      "Iteration:  301  Accuracy: 0.9652\n",
      "Iteration:  302  Accuracy: 0.9648\n",
      "Iteration:  303  Accuracy: 0.9652\n",
      "Iteration:  304  Accuracy: 0.9653\n",
      "Iteration:  305  Accuracy: 0.9647\n",
      "Iteration:  306  Accuracy: 0.9651\n",
      "Iteration:  307  Accuracy: 0.9651\n",
      "Iteration:  308  Accuracy: 0.9649\n",
      "Iteration:  309  Accuracy: 0.9652\n",
      "Iteration:  310  Accuracy: 0.9653\n",
      "Iteration:  311  Accuracy: 0.9651\n",
      "Iteration:  312  Accuracy: 0.9648\n",
      "Iteration:  313  Accuracy: 0.9654\n",
      "Iteration:  314  Accuracy: 0.9653\n",
      "Iteration:  315  Accuracy: 0.9653\n",
      "Iteration:  316  Accuracy: 0.9656\n",
      "Iteration:  317  Accuracy: 0.9651\n",
      "Iteration:  318  Accuracy: 0.9656\n",
      "Iteration:  319  Accuracy: 0.9655\n",
      "Iteration:  320  Accuracy: 0.9657\n",
      "Iteration:  321  Accuracy: 0.9657\n",
      "Iteration:  322  Accuracy: 0.9659\n",
      "Iteration:  323  Accuracy: 0.9656\n",
      "Iteration:  324  Accuracy: 0.9654\n",
      "Iteration:  325  Accuracy: 0.9657\n",
      "Iteration:  326  Accuracy: 0.966\n",
      "Iteration:  327  Accuracy: 0.9656\n",
      "Iteration:  328  Accuracy: 0.9661\n",
      "Iteration:  329  Accuracy: 0.9657\n",
      "Iteration:  330  Accuracy: 0.9659\n",
      "Iteration:  331  Accuracy: 0.966\n",
      "Iteration:  332  Accuracy: 0.9661\n",
      "Iteration:  333  Accuracy: 0.9664\n",
      "Iteration:  334  Accuracy: 0.9662\n",
      "Iteration:  335  Accuracy: 0.9662\n",
      "Iteration:  336  Accuracy: 0.9663\n",
      "Iteration:  337  Accuracy: 0.9664\n",
      "Iteration:  338  Accuracy: 0.9659\n",
      "Iteration:  339  Accuracy: 0.9667\n",
      "Iteration:  340  Accuracy: 0.9666\n",
      "Iteration:  341  Accuracy: 0.9664\n",
      "Iteration:  342  Accuracy: 0.9665\n",
      "Iteration:  343  Accuracy: 0.9665\n",
      "Iteration:  344  Accuracy: 0.9665\n",
      "Iteration:  345  Accuracy: 0.9664\n",
      "Iteration:  346  Accuracy: 0.9667\n",
      "Iteration:  347  Accuracy: 0.9664\n",
      "Iteration:  348  Accuracy: 0.9672\n",
      "Iteration:  349  Accuracy: 0.9667\n",
      "Iteration:  350  Accuracy: 0.967\n",
      "Iteration:  351  Accuracy: 0.9673\n",
      "Iteration:  352  Accuracy: 0.9671\n",
      "Iteration:  353  Accuracy: 0.967\n",
      "Iteration:  354  Accuracy: 0.9668\n",
      "Iteration:  355  Accuracy: 0.9672\n",
      "Iteration:  356  Accuracy: 0.9675\n",
      "Iteration:  357  Accuracy: 0.967\n",
      "Iteration:  358  Accuracy: 0.9678\n",
      "Iteration:  359  Accuracy: 0.9674\n",
      "Iteration:  360  Accuracy: 0.9671\n",
      "Iteration:  361  Accuracy: 0.9671\n",
      "Iteration:  362  Accuracy: 0.9669\n",
      "Iteration:  363  Accuracy: 0.9674\n",
      "Iteration:  364  Accuracy: 0.9675\n",
      "Iteration:  365  Accuracy: 0.9675\n",
      "Iteration:  366  Accuracy: 0.9675\n",
      "Iteration:  367  Accuracy: 0.9677\n",
      "Iteration:  368  Accuracy: 0.9675\n",
      "Iteration:  369  Accuracy: 0.9677\n",
      "Iteration:  370  Accuracy: 0.9679\n",
      "Iteration:  371  Accuracy: 0.9682\n",
      "Iteration:  372  Accuracy: 0.9675\n",
      "Iteration:  373  Accuracy: 0.9678\n",
      "Iteration:  374  Accuracy: 0.9678\n",
      "Iteration:  375  Accuracy: 0.9677\n",
      "Iteration:  376  Accuracy: 0.9681\n",
      "Iteration:  377  Accuracy: 0.9681\n",
      "Iteration:  378  Accuracy: 0.9681\n",
      "Iteration:  379  Accuracy: 0.9677\n",
      "Iteration:  380  Accuracy: 0.9681\n",
      "Iteration:  381  Accuracy: 0.9682\n",
      "Iteration:  382  Accuracy: 0.9681\n",
      "Iteration:  383  Accuracy: 0.968\n",
      "Iteration:  384  Accuracy: 0.9684\n",
      "Iteration:  385  Accuracy: 0.9682\n",
      "Iteration:  386  Accuracy: 0.968\n",
      "Iteration:  387  Accuracy: 0.9684\n",
      "Iteration:  388  Accuracy: 0.9681\n",
      "Iteration:  389  Accuracy: 0.9686\n",
      "Iteration:  390  Accuracy: 0.9685\n",
      "Iteration:  391  Accuracy: 0.9686\n",
      "Iteration:  392  Accuracy: 0.9685\n",
      "Iteration:  393  Accuracy: 0.9685\n",
      "Iteration:  394  Accuracy: 0.9686\n",
      "Iteration:  395  Accuracy: 0.9686\n",
      "Iteration:  396  Accuracy: 0.9687\n",
      "Iteration:  397  Accuracy: 0.9687\n",
      "Iteration:  398  Accuracy: 0.9685\n",
      "Iteration:  399  Accuracy: 0.9684\n",
      "Iteration:  400  Accuracy: 0.9687\n",
      "Iteration:  401  Accuracy: 0.9689\n",
      "Iteration:  402  Accuracy: 0.9688\n",
      "Iteration:  403  Accuracy: 0.9689\n",
      "Iteration:  404  Accuracy: 0.9691\n",
      "Iteration:  405  Accuracy: 0.9688\n",
      "Iteration:  406  Accuracy: 0.969\n",
      "Iteration:  407  Accuracy: 0.969\n",
      "Iteration:  408  Accuracy: 0.9693\n",
      "Iteration:  409  Accuracy: 0.9692\n",
      "Iteration:  410  Accuracy: 0.969\n",
      "Iteration:  411  Accuracy: 0.969\n",
      "Iteration:  412  Accuracy: 0.9693\n",
      "Iteration:  413  Accuracy: 0.9693\n",
      "Iteration:  414  Accuracy: 0.9693\n",
      "Iteration:  415  Accuracy: 0.9693\n",
      "Iteration:  416  Accuracy: 0.9698\n",
      "Iteration:  417  Accuracy: 0.9693\n",
      "Iteration:  418  Accuracy: 0.9694\n",
      "Iteration:  419  Accuracy: 0.9694\n",
      "Iteration:  420  Accuracy: 0.9697\n",
      "Iteration:  421  Accuracy: 0.9697\n",
      "Iteration:  422  Accuracy: 0.9698\n",
      "Iteration:  423  Accuracy: 0.9695\n",
      "Iteration:  424  Accuracy: 0.9697\n",
      "Iteration:  425  Accuracy: 0.9694\n",
      "Iteration:  426  Accuracy: 0.97\n",
      "Iteration:  427  Accuracy: 0.9699\n",
      "Iteration:  428  Accuracy: 0.9696\n",
      "Iteration:  429  Accuracy: 0.9697\n",
      "Iteration:  430  Accuracy: 0.9699\n",
      "Iteration:  431  Accuracy: 0.97\n",
      "Iteration:  432  Accuracy: 0.97\n",
      "Iteration:  433  Accuracy: 0.9704\n",
      "Iteration:  434  Accuracy: 0.9698\n",
      "Iteration:  435  Accuracy: 0.9699\n",
      "Iteration:  436  Accuracy: 0.97\n",
      "Iteration:  437  Accuracy: 0.97\n",
      "Iteration:  438  Accuracy: 0.97\n",
      "Iteration:  439  Accuracy: 0.9704\n",
      "Iteration:  440  Accuracy: 0.97\n",
      "Iteration:  441  Accuracy: 0.9707\n",
      "Iteration:  442  Accuracy: 0.97\n",
      "Iteration:  443  Accuracy: 0.9705\n",
      "Iteration:  444  Accuracy: 0.9705\n",
      "Iteration:  445  Accuracy: 0.9705\n",
      "Iteration:  446  Accuracy: 0.9701\n",
      "Iteration:  447  Accuracy: 0.9703\n",
      "Iteration:  448  Accuracy: 0.9705\n",
      "Iteration:  449  Accuracy: 0.9703\n",
      "Iteration:  450  Accuracy: 0.9705\n",
      "Iteration:  451  Accuracy: 0.9709\n",
      "Iteration:  452  Accuracy: 0.9708\n",
      "Iteration:  453  Accuracy: 0.9707\n",
      "Iteration:  454  Accuracy: 0.9708\n",
      "Iteration:  455  Accuracy: 0.9708\n",
      "Iteration:  456  Accuracy: 0.9706\n",
      "Iteration:  457  Accuracy: 0.9708\n",
      "Iteration:  458  Accuracy: 0.9708\n",
      "Iteration:  459  Accuracy: 0.9707\n",
      "Iteration:  460  Accuracy: 0.9708\n",
      "Iteration:  461  Accuracy: 0.9708\n",
      "Iteration:  462  Accuracy: 0.9709\n",
      "Iteration:  463  Accuracy: 0.9709\n",
      "Iteration:  464  Accuracy: 0.9708\n",
      "Iteration:  465  Accuracy: 0.9709\n",
      "Iteration:  466  Accuracy: 0.9709\n",
      "Iteration:  467  Accuracy: 0.9708\n",
      "Iteration:  468  Accuracy: 0.9708\n",
      "Iteration:  469  Accuracy: 0.971\n",
      "Iteration:  470  Accuracy: 0.9708\n",
      "Iteration:  471  Accuracy: 0.9709\n",
      "Iteration:  472  Accuracy: 0.9707\n",
      "Iteration:  473  Accuracy: 0.9708\n",
      "Iteration:  474  Accuracy: 0.9707\n",
      "Iteration:  475  Accuracy: 0.971\n",
      "Iteration:  476  Accuracy: 0.9708\n",
      "Iteration:  477  Accuracy: 0.9708\n",
      "Iteration:  478  Accuracy: 0.9713\n",
      "Iteration:  479  Accuracy: 0.971\n",
      "Iteration:  480  Accuracy: 0.9715\n",
      "Iteration:  481  Accuracy: 0.971\n",
      "Iteration:  482  Accuracy: 0.9713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  483  Accuracy: 0.971\n",
      "Iteration:  484  Accuracy: 0.9713\n",
      "Iteration:  485  Accuracy: 0.9712\n",
      "Iteration:  486  Accuracy: 0.9712\n",
      "Iteration:  487  Accuracy: 0.9712\n",
      "Iteration:  488  Accuracy: 0.9711\n",
      "Iteration:  489  Accuracy: 0.9711\n",
      "Iteration:  490  Accuracy: 0.9714\n",
      "Iteration:  491  Accuracy: 0.9709\n",
      "Iteration:  492  Accuracy: 0.9712\n",
      "Iteration:  493  Accuracy: 0.9713\n",
      "Iteration:  494  Accuracy: 0.9713\n",
      "Iteration:  495  Accuracy: 0.9711\n",
      "Iteration:  496  Accuracy: 0.9713\n",
      "Iteration:  497  Accuracy: 0.9711\n",
      "Iteration:  498  Accuracy: 0.9714\n",
      "Iteration:  499  Accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dounload the MNIST dataset and save it in MNIST_data folder.\n",
    "# one_hot is One_hot means that for each data point we create an array the size of the classes, fill the array with '0',\n",
    "# and only in the class whose data point belongs to it we will denote '1'.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "(hidden1_size, hidden2_size) = (100, 50)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, hidden1_size], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "z1 = tf.nn.relu(tf.matmul(x,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "z2 = tf.nn.relu(tf.matmul(z1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden2_size, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(z2, W3) + b3)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "iteration_axis = np.array([])\n",
    "accuracy_axis = np.array([])\n",
    "\n",
    "for i in range (500):\n",
    "    for _ in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    iteration_axis = np.append(iteration_axis, [i], axis=0)\n",
    "    accuracy_axis = np.append(accuracy_axis, [sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})], axis=0)\n",
    "    print(\"Iteration: \",i, \" Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of this network is 784 neurons in the input layer, 100 neurons in the first hidden layer, 50 neurons in the second hidden layer and 10 neuron in the output layer. And finally put the outputs to softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iteration_axis, accuracy_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^ Contents](#contents)\n",
    "## Resolving Errors\n",
    "<a id='resolving_errors'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If train error is too high:\n",
    "* Error in code\n",
    "* Extend the training time\n",
    "* Add more layers or more features\n",
    "* Change the $\\alpha$ or the initial values of the weights\n",
    "* Change the activation function (eg: SGD, RMSProp, Adagrad, Adam)\n",
    "\n",
    "If test error is too high (but train-error is ok):\n",
    "* Add more data to dataset\n",
    "* Use early stopping\n",
    "* Use regularization\n",
    "    * Dropout\n",
    "    * Ridge/LASSO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
